{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a03e0a8e",
   "metadata": {},
   "source": [
    "**Next method**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "11ad2f29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\edward_b\\OneDrive - Institute for Fiscal Studies\\Work\\Brazil social insurance\\venv\\Scripts\\python.exe\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8f14978",
   "metadata": {},
   "source": [
    "***Pyautogui***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c828cf6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyautogui\n",
    "import os\n",
    "import time\n",
    "import subprocess\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from bs4 import BeautifulSoup\n",
    "import psutil\n",
    "from selenium.webdriver.support.ui import Select\n",
    "import matplotlib.pyplot as plt\n",
    "import logging\n",
    "from logging.handlers import RotatingFileHandler\n",
    "import random\n",
    "import requests\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0797de06",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"C:/Users/edward_b/OneDrive - Institute for Fiscal Studies/Work/Brazil social insurance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3204df2",
   "metadata": {},
   "source": [
    "**Define functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cc4f9aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to kill Chrome processes\n",
    "def kill_chrome():\n",
    "    \"\"\"Kill all Chrome processes.\"\"\"\n",
    "    for proc in psutil.process_iter(['pid', 'name']):\n",
    "        if 'chrome' in proc.info['name'].lower():\n",
    "            try:\n",
    "                proc.kill()\n",
    "            except psutil.NoSuchProcess:\n",
    "                pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "51eb8b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def scrape_data(cnpj, year, soup, table):\n",
    "    # Step 1: Extract column headers (excluding unwanted labels)\n",
    "    header_rows = table.find('thead').find_all('tr')\n",
    "    cols = []\n",
    "    for row in header_rows:\n",
    "        headers = [th.get_text(strip=True) for th in row.find_all(\"th\") if th.get_text(strip=True)]\n",
    "        filtered = [h for h in headers if h != \"Resumo do DAS a ser gerado\"]\n",
    "        cols.extend(filtered)\n",
    "    print(f\"Extracted headers: {cols}\")\n",
    "\n",
    "    # Check if \"Quotas\" is in the headers to determine if we need to split rows\n",
    "    quota_split = \"Quotas\" in cols\n",
    "\n",
    "    # Step 2: Find all relevant data rows\n",
    "    rows = soup.find_all(\"tr\", class_=\"pa\")\n",
    "\n",
    "    # Step 3: Process data rows with split-row logic\n",
    "    cleaned_data = []\n",
    "    i = 0\n",
    "    while i < len(rows):\n",
    "        row = rows[i]\n",
    "        cells = row.find_all(\"td\")\n",
    "\n",
    "        # Extract visible text from the cells (skipping the first <td> with checkbox)\n",
    "        cell_texts = [td.get_text(strip=True) for td in cells[1:]]\n",
    "\n",
    "        if quota_split: #If we have a table with a quotas column\n",
    "        \n",
    "            # Check if each row  has quotas that require a split\n",
    "            quota_row = any(\n",
    "                inp.get(\"data-pa-quota\") == \"true\"\n",
    "                for inp in row.find_all(\"input\", attrs={\"data-pa-quota\": True})\n",
    "            )\n",
    "\n",
    "            if quota_row:\n",
    "                # First 4 cells: Período, Apurado, Benefício, Quotas (set to 1)\n",
    "                base_info = cell_texts[:4]\n",
    "                base_info[3] = \"1\" if quota_split else \"0\"\n",
    "                payment_data = cell_texts[4:]\n",
    "                cleaned_data.append(base_info + payment_data)\n",
    "\n",
    "                # Append next row with same identifying info if exists\n",
    "                if i + 1 < len(rows):\n",
    "                    next_row = rows[i + 1]\n",
    "                    next_cells = next_row.find_all(\"td\")\n",
    "                    next_texts = [td.get_text(strip=True) for td in next_cells]\n",
    "\n",
    "                    cleaned_data.append(base_info + next_texts)\n",
    "                    i += 2\n",
    "                else:\n",
    "                    i += 1\n",
    "            else:\n",
    "                # Normal row within a quotas table, treat quotas as 0 if not explicitly set\n",
    "                if len(cell_texts) >= 5:\n",
    "                    cell_texts[3] = \"0\"\n",
    "                cleaned_data.append(cell_texts)\n",
    "                i += 1\n",
    "\n",
    "        # Normal table without quotas\n",
    "        else:    \n",
    "            cell_data = [cell.get_text(strip=True) for cell in cells[1:]]  # Skip the first cell\n",
    "            cleaned_data.append(cell_data)\n",
    "            i += 1\n",
    "\n",
    "    \n",
    "    # Step 4: Build DataFrame\n",
    "    df = pd.DataFrame(cleaned_data, columns=cols)\n",
    "    df['cnpj'] = cnpj\n",
    "    df['data_found'] = True\n",
    "\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e30ea966",
   "metadata": {},
   "source": [
    "**Import master**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f346739",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"raw/CNPJ numbers\"\n",
    "cnpj_master = pd.read_csv(f'{path}/simples.csv', sep=',', encoding='utf-8')\n",
    "cnpj_master = cnpj_master[['cnpj_basico','opcao_mei']]\n",
    "\n",
    "# find length of cnpj\n",
    "cnpj_master[\"length cnpj_basico\"] = cnpj_master[\"cnpj_basico\"].astype(str).str.len()\n",
    "pd.crosstab(cnpj_master[\"length cnpj_basico\"], cnpj_master[\"opcao_mei\"], margins=True, margins_name=\"Total\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba0f732b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnpj_master[\"cnpj_basico\"] = cnpj_master[\"cnpj_basico\"].astype(str)\n",
    "cnpj_master = cnpj_master[cnpj_master['opcao_mei'] == 1]\n",
    "cnpj_master.drop(columns=['length cnpj_basico'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "693e2916",
   "metadata": {},
   "source": [
    "**Import example full file**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7abcd84",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnpj_test = pd.read_csv(f'{path}/establishmentsPI.csv', sep=',', encoding='utf-8')\n",
    "#cnpj_test = cnpj_test[['cnpj']]\n",
    "cnpj_test[\"cnpj_basico\"] = cnpj_test[\"cnpj\"].astype(str).str[:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c01d883",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnpj_merged = pd.merge(cnpj_master, cnpj_test, left_on='cnpj_basico', right_on='cnpj_basico', how='inner')\n",
    "cnpj_merged[\"cnpj\"] = cnpj_merged[\"cnpj\"].astype(str)\n",
    "#cnpj_merged[\"cnpj\"].str.len().hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cbf108e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnpj_merged = cnpj_merged[cnpj_merged['cnpj'].str.len() == 14]\n",
    "\n",
    "#Check for duplicates\n",
    "duplicates = cnpj_merged[cnpj_merged['cnpj'].duplicated(keep=False)]\n",
    "print(len(duplicates) == 0)\n",
    "cnpj_merged = cnpj_merged.drop_duplicates(subset=['cnpj'], keep='first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89e4890a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnpj_merged.to_csv('MEI_numbers.csv', sep=',', encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46d10065",
   "metadata": {},
   "source": [
    "*****-----------------------Load in MEI numbers and start scraping-----------------------*****"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c7dac715",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnpj_merged = pd.read_csv('MEI_numbers.csv', sep=',', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a089f127",
   "metadata": {},
   "outputs": [],
   "source": [
    "# proxy_list = pd.read_csv(\"scripts/proxies.txt\", sep=',', encoding='utf-8', header=None, names=['proxy'])\n",
    "# proxy_list = proxy_list[\"proxy\"].to_list()\n",
    "# proxy_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "355e8e64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 3)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get random sample of 10\n",
    "cnpj_merged = cnpj_merged.sample(n=100, random_state=10)\n",
    "#convert cnpj's to a list\n",
    "cnpj_list = cnpj_merged['cnpj'].tolist()\n",
    "cnpj_merged.shape # 13,894 obs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e69f6e81",
   "metadata": {},
   "source": [
    "**Check if proxies work**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "606b1843",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# proxy = \"66.201.7.151\"  # Replace with the proxy you want to test\n",
    "\n",
    "# try:\n",
    "#     response = requests.get(\n",
    "#         \"https://httpbin.org/ip\",\n",
    "#         proxies={\"http\": proxy, \"https\": proxy},\n",
    "#         timeout=10\n",
    "#     )\n",
    "#     print(\"Proxy is working. IP seen by server:\", response.text)\n",
    "# except Exception as e:\n",
    "#     print(\"Proxy failed:\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d37d94dd",
   "metadata": {},
   "source": [
    "**Set up scraper**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b718830",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnpj_list = [str(i) for i in cnpj_list]\n",
    "cnpj_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "93310a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnpj_list = [\"35184782000140\", \"31898571000119\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e92761c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import os\n",
    "\n",
    "chrome_profile_path = \"C:/Temp/ChromeDebug\"\n",
    "if os.path.exists(chrome_profile_path):\n",
    "    shutil.rmtree(chrome_profile_path)  # delete the profile folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e6003240",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_file = 'mei_scraper_log.log'\n",
    "with open(log_file, 'w'):\n",
    "    pass  # This empties the file\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        RotatingFileHandler(log_file, maxBytes=5*1024*1024, backupCount=3),  # 5MB per file\n",
    "        logging.StreamHandler()  # Optional: print to console too\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cacf97e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Define url and cnpj list ----\n",
    "url = \"https://www8.receita.fazenda.gov.br/SimplesNacional/Aplicacoes/ATSPO/pgmei.app/Identificacao\"\n",
    "data = []\n",
    "timings = []\n",
    "total_start_time = time.time()\n",
    "master_df = pd.DataFrame() \n",
    "\n",
    "\n",
    "for cnpj in cnpj_list:\n",
    "    #proxy = random.choice(proxy_list)\n",
    "    logging.info(\"Processing CNPJ:\", cnpj)\n",
    "    start_time = time.time()\n",
    "\n",
    "    try:\n",
    "\n",
    "        # ---- Step 1: Start Chrome in remote debug mode ----\n",
    "        subprocess.Popen([\n",
    "            r\"C:/Program Files/Google/Chrome/Application/chrome.exe\",\n",
    "            #f\"--proxy-server={proxy}\",\n",
    "            \"--remote-debugging-port=9222\",\n",
    "            \"--user-data-dir=\" + chrome_profile_path,\n",
    "            \"--start-maximized\",  # or \"--start-fullscreen\"\n",
    "            \"--disable-popup-blocking\",  # optional, disable for debugging only\n",
    "            \"--disable-extensions\",\n",
    "            \"--no-first-run\",\n",
    "            \"--no-default-browser-check\"\n",
    "        ])\n",
    "        time.sleep(2)  # Give Chrome time to launch\n",
    "\n",
    "        # ---- Step 2: Use pyautogui to interact with the site ----\n",
    "        pyautogui.hotkey('ctrl', 'l')\n",
    "        pyautogui.typewrite(url, interval=0.01)\n",
    "        pyautogui.press('enter')  \n",
    "        time.sleep(2)\n",
    "\n",
    "        pyautogui.moveTo(x=1027, y=377, duration=1) # laptop x=722, y=391\n",
    "        pyautogui.click()\n",
    "        pyautogui.typewrite(cnpj, interval=0.1)\n",
    "\n",
    "        pyautogui.moveTo(x=1027, y=500, duration=1) # Laptop x=722, y=514\n",
    "        pyautogui.click()\n",
    "        time.sleep(2)\n",
    "\n",
    "        options = Options()\n",
    "        options.add_experimental_option(\"debuggerAddress\", \"127.0.0.1:9222\")\n",
    "        #options.add_argument(f'--proxy-server={proxy}')\n",
    "        driver = webdriver.Chrome(options=options)\n",
    "        wait = WebDriverWait(driver, 3)\n",
    "\n",
    "        driver.get(\"https://www8.receita.fazenda.gov.br/SimplesNacional/Aplicacoes/ATSPO/pgmei.app/emissao\")\n",
    "        time.sleep(1.5)\n",
    "\n",
    "        # First try: Bootstrap-styled dropdown\n",
    "        try:\n",
    "            dropdown_button = wait.until(EC.element_to_be_clickable((By.CSS_SELECTOR, 'button[data-id=\"anoCalendarioSelect\"]')))\n",
    "            dropdown_button.click()\n",
    "            time.sleep(1)\n",
    "\n",
    "            year_elements = wait.until(EC.presence_of_all_elements_located(\n",
    "                (By.CSS_SELECTOR, \".dropdown-menu.inner li a span.text\")\n",
    "            ))\n",
    "            enabled_years = [el.text.strip() for el in year_elements if el.text.strip()]\n",
    "            # remove elements from the list that contain \"Não optante\"\n",
    "            enabled_years = [year for year in enabled_years if \"Não optante\" not in year]\n",
    "\n",
    "            # Raise an exception if no enabled years are found\n",
    "            if not enabled_years:\n",
    "                raise ValueError(\"No enabled years found in the dropdown menu.\")\n",
    "\n",
    "            print(\"Bootstrap dropdown enabled years for CNPJ \", cnpj , \":\", enabled_years)\n",
    "            use_bootstrap = True\n",
    "            \n",
    "\n",
    "        except Exception as e:\n",
    "            print(\"Bootstrap dropdown failed, falling back to native <select> method.\")\n",
    "            # Try native <select>\n",
    "            select_element = wait.until(EC.presence_of_element_located((By.ID, \"anoCalendarioSelect\")))\n",
    "            dropdown = Select(select_element)\n",
    "            enabled_years = [o.text.strip() for o in dropdown.options if o.text.strip()]\n",
    "            enabled_years = [year for year in enabled_years if \"Não optante\" not in year]\n",
    "            print(\"Native <select> enabled years for CNPJ \", cnpj,\":\", enabled_years)\n",
    "            use_bootstrap = False\n",
    "\n",
    "        print(\"scraping years\", enabled_years)\n",
    "        #enabled_years = [str(max(enabled_years))]\n",
    "        enabled_years.insert(0, \"2010\")  #add a year to the start of the list\n",
    "        \n",
    "        for index, year in enumerate(enabled_years):\n",
    "            try:\n",
    "                if use_bootstrap:\n",
    "                    dropdown_button = wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, 'button[data-id=\"anoCalendarioSelect\"]')))\n",
    "                    driver.execute_script(\"arguments[0].click();\", dropdown_button)\n",
    "                    time.sleep(1.5)  # allow dropdown to render\n",
    "                    \n",
    "                    print(\"Clicking on year:\", year)\n",
    "                    year_option = wait.until(EC.element_to_be_clickable(\n",
    "                        (By.XPATH, f\"//span[@class='text' and normalize-space(text())='{year}']\")\n",
    "                    ))\n",
    "                    time.sleep(2)\n",
    "                    driver.execute_script(\"arguments[0].click();\", year_option)\n",
    "                    #ActionChains(driver).move_to_element(year_option).click().perform()\n",
    "                    print(f\"Selected (Bootstrap) year: {year}\")\n",
    "                else:\n",
    "                    dropdown = Select(driver.find_element(By.ID, \"anoCalendarioSelect\"))\n",
    "                    dropdown.select_by_visible_text(year)\n",
    "                    print(f\"Selected (native) year: {year}\")\n",
    "\n",
    "                ok_button = driver.find_element(By.CSS_SELECTOR, \"button[type='submit']\")\n",
    "                ok_button.click()\n",
    "                time.sleep(2)\n",
    "                \n",
    "                # check if table is present\n",
    "                soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "                table = soup.find('table', class_='table table-hover table-condensed emissao is-detailed')\n",
    "                if not table:\n",
    "                    print(f\"No table found for {cnpj} in year {year}. Skipping the rest.\")\n",
    "                    # Mark all remaining years as not found\n",
    "                    for remaining_year in enabled_years[index:]:\n",
    "                        data.append({ ############ Added this line to store data for remaining years but in line with pandas dataframe\n",
    "                            'cnpj': cnpj,\n",
    "                            'Período de Apuração': remaining_year,\n",
    "                            'data_found': False\n",
    "                        })\n",
    "                        # Convert missing data to DataFrame and append to master_df\n",
    "                    missing_df = pd.DataFrame(data)\n",
    "                    master_df = pd.concat([master_df, missing_df], ignore_index=True)\n",
    "                    break  # Exit the year loop\n",
    "                    \n",
    "                new_data = scrape_data(cnpj, year, soup, table)\n",
    "                master_df = pd.concat([master_df, new_data], ignore_index=True)\n",
    "\n",
    "                driver.back()\n",
    "                time.sleep(2)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error with year {year}:\", e)\n",
    "\n",
    "    except Exception as outer_error:\n",
    "        print(f\"Fatal error with CNPJ {cnpj}:\", outer_error)\n",
    "\n",
    "    finally:\n",
    "        try:\n",
    "            driver.quit()\n",
    "        except:\n",
    "            pass\n",
    "        kill_chrome()\n",
    "        end_time = time.time()  # ⏱ End timer\n",
    "        elapsed = end_time - start_time\n",
    "        timings.append(elapsed)\n",
    "        total_elapsed = time.time() - total_start_time\n",
    "        average_elapsed = sum(timings) / len(timings)\n",
    "        \n",
    "        logging.info(f\"Finished CNPJ: {cnpj} in {elapsed:.2f} seconds\\n\")\n",
    "        logging.info(f\"Average time per CNPJ: {average_elapsed:.2f} seconds\")\n",
    "        logging.info(f\"Total time elapsed: {total_elapsed:.2f} seconds\\n\")\n",
    "        time.sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4018a1b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "master_df = master_df[['cnpj', 'Período de Apuração', 'Apurado', 'Situação', 'Benefício INSS',\n",
    "         'Quotas', 'Principal', 'Multa', 'Juros', 'Total',\n",
    "         'Data de Vencimento', 'Data de Acolhimento', 'data_found']]\n",
    "\n",
    "# replace quotas with 0 if it is NaN\n",
    "master_df['Quotas'] = master_df['Quotas'].fillna(0).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a86cfe31",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(data).to_csv('MEI_data.csv', sep=',', encoding='utf-8', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

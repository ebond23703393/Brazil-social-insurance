{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a03e0a8e",
   "metadata": {},
   "source": [
    "**Next method**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "11ad2f29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\edward_b\\OneDrive - Institute for Fiscal Studies\\Work\\Brazil social insurance\\venv\\Scripts\\python.exe\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f11eaa39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\edward_b\\OneDrive - Institute for Fiscal Studies\\Work\\Brazil social insurance\\scripts\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8f14978",
   "metadata": {},
   "source": [
    "***Pyautogui***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c828cf6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyautogui\n",
    "import time\n",
    "import subprocess\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from bs4 import BeautifulSoup\n",
    "import psutil\n",
    "from selenium.webdriver.support.ui import Select\n",
    "import matplotlib.pyplot as plt\n",
    "import logging\n",
    "from logging.handlers import RotatingFileHandler\n",
    "import random\n",
    "import requests\n",
    "import re\n",
    "import shutil\n",
    "import os\n",
    "\n",
    "from utils import extract_cpf, kill_chrome, cnpj_check, scrape_data, obtain_pdf, debt_collector, outstanding_payment, scrape_debt_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0797de06",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"C:/Users/edward_b/OneDrive - Institute for Fiscal Studies/Work/Brazil social insurance\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e30ea966",
   "metadata": {},
   "source": [
    "**Import master**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5f346739",
   "metadata": {},
   "outputs": [],
   "source": [
    "# path = \"raw/CNPJ numbers\"\n",
    "# cnpj_master = pd.read_csv(f'{path}/simples.csv', sep=',', encoding='utf-8')\n",
    "# cnpj_master = cnpj_master[['cnpj_basico','opcao_mei']]\n",
    "\n",
    "# # find length of cnpj\n",
    "# cnpj_master[\"length cnpj_basico\"] = cnpj_master[\"cnpj_basico\"].astype(str).str.len()\n",
    "# pd.crosstab(cnpj_master[\"length cnpj_basico\"], cnpj_master[\"opcao_mei\"], margins=True, margins_name=\"Total\")\n",
    "\n",
    "# cnpj_master[\"cnpj_basico\"] = cnpj_master[\"cnpj_basico\"].astype(str)\n",
    "# cnpj_master = cnpj_master[cnpj_master['opcao_mei'] == 1]\n",
    "# cnpj_master.drop(columns=['length cnpj_basico'], inplace=True)\n",
    "\n",
    "# cnpj_test = pd.read_csv(f'{path}/establishmentsPI.csv', sep=',', encoding='utf-8')\n",
    "# #cnpj_test = cnpj_test[['cnpj']]\n",
    "# cnpj_test[\"cnpj_basico\"] = cnpj_test[\"cnpj\"].astype(str).str[:8]\n",
    "\n",
    "# cnpj_merged = pd.merge(cnpj_master, cnpj_test, left_on='cnpj_basico', right_on='cnpj_basico', how='inner')\n",
    "# cnpj_merged[\"cnpj\"] = cnpj_merged[\"cnpj\"].astype(str)\n",
    "# #cnpj_merged[\"cnpj\"].str.len().hist()\n",
    "\n",
    "# cnpj_merged = cnpj_merged[cnpj_merged['cnpj'].str.len() == 14]\n",
    "\n",
    "# #Check for duplicates\n",
    "# duplicates = cnpj_merged[cnpj_merged['cnpj'].duplicated(keep=False)]\n",
    "# print(len(duplicates) == 0)\n",
    "# cnpj_merged = cnpj_merged.drop_duplicates(subset=['cnpj'], keep='first')\n",
    "\n",
    "# cnpj_merged.to_csv('MEI_numbers.csv', sep=',', encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46d10065",
   "metadata": {},
   "source": [
    "*****-----------------------Load in MEI numbers and start scraping-----------------------*****"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c7dac715",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnpj_merged = pd.read_csv('MEI_numbers.csv', sep=',', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a089f127",
   "metadata": {},
   "outputs": [],
   "source": [
    "# proxy_list = pd.read_csv(\"scripts/proxies.txt\", sep=',', encoding='utf-8', header=None, names=['proxy'])\n",
    "# proxy_list = proxy_list[\"proxy\"].to_list()\n",
    "# proxy_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "355e8e64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 3)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get random sample of 10\n",
    "cnpj_merged = cnpj_merged.sample(n=5, random_state=10)\n",
    "#convert cnpj's to a list\n",
    "cnpj_list = cnpj_merged['cnpj'].tolist()\n",
    "cnpj_merged.shape # 13,894 obs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d37d94dd",
   "metadata": {},
   "source": [
    "**Set up scraper**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0b718830",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['28740844000198',\n",
       " '11757216000112',\n",
       " '33962463000193',\n",
       " '31898571000119',\n",
       " '12902387000150']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnpj_list = [str(i) for i in cnpj_list]\n",
    "cnpj_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "93310a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnpj_list = [\"35184782000140\"] #,\"33962463000193\", \"31898571000119\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e92761c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import os\n",
    "\n",
    "chrome_profile_path = \"C:/Temp/ChromeDebug\"\n",
    "if os.path.exists(chrome_profile_path):\n",
    "    shutil.rmtree(chrome_profile_path)  # delete the profile folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e6003240",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_file = 'mei_scraper_log.log'\n",
    "with open(log_file, 'w'):\n",
    "    pass  # This empties the file\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        RotatingFileHandler(log_file, maxBytes=5*1024*1024, backupCount=3),  # 5MB per file\n",
    "        logging.StreamHandler()  # Optional: print to console too\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "57df1911",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Point(x=1749, y=341)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pyautogui.position()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "357d17e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Define url and cnpj list ----\n",
    "url = \"https://www8.receita.fazenda.gov.br/SimplesNacional/Aplicacoes/ATSPO/pgmei.app/Identificacao\"\n",
    "\n",
    "timings = []\n",
    "total_start_time = time.time()\n",
    "master_df = pd.DataFrame()\n",
    "master_debt_df = pd.DataFrame()  # DataFrame to store debt collection data\n",
    "\n",
    "\n",
    "for cnpj in cnpj_list:\n",
    "    #proxy = random.choice_list)\n",
    "    #logging.info(\"Processing CNPJ:\", cnpj)\n",
    "    start_time = time.time()\n",
    "    data = []\n",
    "\n",
    "    chrome_profile_path = \"C:/Temp/ChromeDebug\"\n",
    "    if os.path.exists(chrome_profile_path):\n",
    "        shutil.rmtree(chrome_profile_path)  # delete the profile folder to avoid Captcha detection. If want to change where downloaded files go, open Chrome in command prompt and change the settings manually.\n",
    "\n",
    "    try:\n",
    "\n",
    "        # ---- Step 1: Start Chrome in remote debug mode ----\n",
    "        subprocess.Popen([\n",
    "            r\"C:/Program Files/Google/Chrome/Application/chrome.exe\",\n",
    "            #f\"--proxy-server={proxy}\",\n",
    "            \"--remote-debugging-port=9222\",\n",
    "            \"--user-data-dir=\" + chrome_profile_path,\n",
    "            \"--start-maximized\",  # or \"--start-fullscreen\"\n",
    "            \"--disable-popup-blocking\",  # optional, disable for debugging only\n",
    "            \"--disable-extensions\",\n",
    "            \"--no-first-run\",\n",
    "            \"--no-default-browser-check\"\n",
    "        ])\n",
    "        time.sleep(2)  # Give Chrome time to launch\n",
    "\n",
    "        # ---- Step 2: Use pyautogui to interact with the site ----\n",
    "        pyautogui.hotkey('ctrl', 'l')\n",
    "        pyautogui.typewrite(url, interval=0.01)\n",
    "        pyautogui.press('enter')  \n",
    "        time.sleep(2)\n",
    "\n",
    "        pyautogui.moveTo(x=1027, y=377, duration=1) # laptop x=722, y=391  deksptop : x=1027, y=377\n",
    "        pyautogui.click()\n",
    "        pyautogui.typewrite(cnpj, interval=0.1)\n",
    "\n",
    "        pyautogui.moveTo(x=1027, y=500, duration=1) # Laptop x=722, y=514 desktop: x=1027, y=500\n",
    "        pyautogui.click()\n",
    "        time.sleep(2)\n",
    "\n",
    "        ## Selenium setup\n",
    "        options = Options()\n",
    "        options.add_experimental_option(\"debuggerAddress\", \"127.0.0.1:9222\")\n",
    "\n",
    "        #options.add_argument(f'--proxy-server={proxy}')\n",
    "        driver = webdriver.Chrome(options=options)\n",
    "        wait = WebDriverWait(driver, 3)\n",
    "\n",
    "        driver.get(\"https://www8.receita.fazenda.gov.br/SimplesNacional/Aplicacoes/ATSPO/pgmei.app/emissao\")\n",
    "        time.sleep(1.5)\n",
    "\n",
    "        # check the cnpj on the webpage is the same as the one we are looking for\n",
    "        cnpj_check(driver, cnpj)\n",
    "\n",
    "        # First try: Bootstrap-styled dropdown\n",
    "        try:\n",
    "            dropdown_button = wait.until(EC.element_to_be_clickable((By.CSS_SELECTOR, 'button[data-id=\"anoCalendarioSelect\"]')))\n",
    "            dropdown_button.click()\n",
    "            time.sleep(1)\n",
    "\n",
    "            year_elements = wait.until(EC.presence_of_all_elements_located(\n",
    "                (By.CSS_SELECTOR, \".dropdown-menu.inner li a span.text\")\n",
    "            ))\n",
    "            enabled_years = [el.text.strip() for el in year_elements if el.text.strip()]\n",
    "            # remove elements from the list that contain \"Não optante\"\n",
    "            enabled_years = [year for year in enabled_years if \"Não optante\" not in year]\n",
    "\n",
    "            # Raise an exception if no enabled years are found\n",
    "            if not enabled_years:\n",
    "                raise ValueError(\"No enabled years found in the dropdown menu.\")\n",
    "\n",
    "            print(\"Bootstrap dropdown enabled years for CNPJ \", cnpj , \":\", enabled_years)\n",
    "            use_bootstrap = True\n",
    "            \n",
    "\n",
    "        except Exception as e:\n",
    "            print(\"Bootstrap dropdown failed, falling back to native <select> method.\")\n",
    "            # Try native <select>\n",
    "            select_element = wait.until(EC.presence_of_element_located((By.ID, \"anoCalendarioSelect\")))\n",
    "            dropdown = Select(select_element)\n",
    "            enabled_years = [o.text.strip() for o in dropdown.options if o.text.strip()]\n",
    "            enabled_years = [year for year in enabled_years if \"Não optante\" not in year]\n",
    "            print(\"Native <select> enabled years for CNPJ \", cnpj,\":\", enabled_years)\n",
    "            use_bootstrap = False\n",
    "\n",
    "        print(\"scraping years\", enabled_years)\n",
    "        #enabled_years = [str(max(enabled_years))] # to just scrape the most recent year\n",
    "        enabled_years.insert(0, \"2010\")  #add a year to the start of the list\n",
    "        \n",
    "        obtained_pdf = 0\n",
    "        for index, year in enumerate(enabled_years):\n",
    "            try:\n",
    "                if use_bootstrap:\n",
    "                    dropdown_button = wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, 'button[data-id=\"anoCalendarioSelect\"]')))\n",
    "                    driver.execute_script(\"arguments[0].click();\", dropdown_button)\n",
    "                    time.sleep(1.5)  # allow dropdown to render\n",
    "                    \n",
    "                    print(\"Clicking on year:\", year)\n",
    "                    year_option = wait.until(EC.element_to_be_clickable(\n",
    "                        (By.XPATH, f\"//span[@class='text' and normalize-space(text())='{year}']\")\n",
    "                    ))\n",
    "                    time.sleep(2)\n",
    "                    driver.execute_script(\"arguments[0].click();\", year_option)\n",
    "                    #ActionChains(driver).move_to_element(year_option).click().perform()\n",
    "                    print(f\"Selected (Bootstrap) year: {year}\")\n",
    "                else:\n",
    "                    dropdown = Select(driver.find_element(By.ID, \"anoCalendarioSelect\"))\n",
    "                    dropdown.select_by_visible_text(year)\n",
    "                    print(f\"Selected (native) year: {year}\")\n",
    "\n",
    "                ok_button = driver.find_element(By.CSS_SELECTOR, \"button[type='submit']\")\n",
    "                ok_button.click()\n",
    "                time.sleep(2)\n",
    "                \n",
    "                # check if table is present\n",
    "                soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "                table = soup.find('table', class_='table table-hover table-condensed emissao is-detailed')\n",
    "                if not table:\n",
    "                    print(f\"No table found for {cnpj} in year {year}. Skipping the rest.\")\n",
    "                    # Mark all remaining years as not found\n",
    "                    for remaining_year in enabled_years[index:]:\n",
    "                        data.append({ \n",
    "                            'cnpj': cnpj,\n",
    "                            'Período de Apuração': remaining_year,\n",
    "                            'data_found': False\n",
    "                        })\n",
    "                        # Convert missing data to DataFrame and append to master_df\n",
    "                    missing_df = pd.DataFrame(data)\n",
    "                    master_df = pd.concat([master_df, missing_df], ignore_index=True)\n",
    "                    break  # Exit the year loop\n",
    "               \n",
    "               #check if the table is subject to debt collection\n",
    "                is_debt_collector = debt_collector(soup)\n",
    "                \n",
    "                # Scrape the main table\n",
    "                new_data = scrape_data(cnpj, year, soup, table)\n",
    "                master_df = pd.concat([master_df, new_data], ignore_index=True)\n",
    "                \n",
    "                if is_debt_collector:\n",
    "                # Also scrape the debt collection table\n",
    "                    print(f\"Debt collection table found in year {year}.\")\n",
    "                    debt_data = scrape_debt_table(cnpj,soup)\n",
    "                    master_debt_df = pd.concat([master_debt_df, debt_data], ignore_index=True)\n",
    "\n",
    "                # Check if outstanding payments exist\n",
    "                outstanding_payments = outstanding_payment(new_data)\n",
    "\n",
    "                # Try to obtain PDF if not in debt collection\n",
    "                if not is_debt_collector and outstanding_payments and not obtained_pdf:\n",
    "                    print(f\"Outstanding payments found in year {year}, period {outstanding_payments}, attempting to obtain PDF.\")\n",
    "                    obtain_pdf(driver,wait, outstanding_payments)\n",
    "                    driver.back()\n",
    "                    obtained_pdf = 1\n",
    "                    \n",
    "                \n",
    "                driver.back()\n",
    "                time.sleep(2)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error with year {year}:\", e)\n",
    "\n",
    "    except Exception as outer_error:\n",
    "        print(f\"Fatal error with CNPJ {cnpj}:\", outer_error)\n",
    "\n",
    "    finally:\n",
    "        try:\n",
    "            driver.quit()\n",
    "        except:\n",
    "            pass\n",
    "        kill_chrome()\n",
    "        end_time = time.time() \n",
    "        elapsed = end_time - start_time\n",
    "        timings.append(elapsed)\n",
    "        total_elapsed = time.time() - total_start_time\n",
    "        average_elapsed = sum(timings) / len(timings)\n",
    "        \n",
    "        logging.info(f\"Finished CNPJ: {cnpj} in {elapsed:.2f} seconds\\n\")\n",
    "        logging.info(f\"Average time per CNPJ: {average_elapsed:.2f} seconds\")\n",
    "        logging.info(f\"Total time elapsed: {total_elapsed:.2f} seconds\\n\")\n",
    "        time.sleep(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Order and Export**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4018a1b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# replace quotas with 0 if it is NaN\n",
    "master_df['Quotas'] = master_df['Quotas'].fillna(0).astype(int)\n",
    "master_df[\"year\"] = master_df[\"Período de Apuração\"].str.extract(r'(\\d{4})').astype(int)\n",
    "#Extract month as text before '/'\n",
    "master_df[\"month\"] = master_df[\"Período de Apuração\"].str.extract(r'(\\w+)')\n",
    "\n",
    "# Convert month to numerical value\n",
    "month_mapping = {\n",
    "    'janeiro': 1, 'fevereiro': 2, 'março': 3, 'abril': 4,\n",
    "    'maio': 5, 'junho': 6, 'julho': 7, 'agosto': 8,\n",
    "    'setembro': 9, 'outubro': 10, 'novembro': 11, 'dezembro': 12\n",
    "}\n",
    "master_df['month'] = master_df['month'].str.lower().map(month_mapping)\n",
    "\n",
    "master_df = master_df[['cnpj', 'Período de Apuração','year','month', 'Apurado', 'Situação', 'Benefício INSS',\n",
    "         'Quotas', 'Principal', 'Multa', 'Juros', 'Total',\n",
    "         'Data de Vencimento', 'Data de Acolhimento', 'data_found']]\n",
    "\n",
    "master_df = master_df.sort_values(by=['cnpj', 'year', 'month'])\n",
    "# pd.DataFrame(data).to_csv('MEI_data.csv', sep=',', encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df70a2e0",
   "metadata": {},
   "source": [
    "**Import PDFs and build database of CNPJ and CPF**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5b67da8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "cnpj",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "cpf",
         "rawType": "object",
         "type": "string"
        }
       ],
       "ref": "94528271-576e-4635-83f4-c831e564e1a3",
       "rows": [
        [
         "0",
         "31898571000119",
         "80283896272"
        ],
        [
         "1",
         "33962463000193",
         "32563388287"
        ],
        [
         "2",
         "35184782000140",
         "00266193269"
        ]
       ],
       "shape": {
        "columns": 2,
        "rows": 3
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cnpj</th>\n",
       "      <th>cpf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>31898571000119</td>\n",
       "      <td>80283896272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>33962463000193</td>\n",
       "      <td>32563388287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>35184782000140</td>\n",
       "      <td>00266193269</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             cnpj          cpf\n",
       "0  31898571000119  80283896272\n",
       "1  33962463000193  32563388287\n",
       "2  35184782000140  00266193269"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## import pdfs\n",
    "pdf_path = os.path.abspath(\"scraped data/pdfs\") # move downloaded pdfs to this directory\n",
    "\n",
    "#load up all pdfs in the pdf_path directory\n",
    "mei_pdf = [os.path.join(pdf_path, f) for f in os.listdir(pdf_path) if f.startswith('DAS-PGMEI')]\n",
    "\n",
    "#extract cpf from pdfs\n",
    "cnpj_cpf = [extract_cpf(i) for i in mei_pdf]\n",
    "cnpj_cpf = pd.DataFrame(cnpj_cpf, columns=['cnpj', 'cpf'])\n",
    "cnpj_cpf"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
